{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1215629",
   "metadata": {},
   "source": [
    "# AAPL Streaming Analysis with Spark SQL & Structured Streaming\n",
    "This project demonstrates real-time factor generation and technical signal construction...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c5adbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.7.189:4041\n",
       "SparkContext available as 'sc' (version = 3.5.2, master = local[*], app id = local-1732312156598)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.catalyst.ScalaReflection\n",
       "import org.apache.spark.sql.types.StructType\n",
       "import org.apache.spark.sql.expressions._\n",
       "import org.apache.spark.sql.functions.udf\n",
       "windowSpec1: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@6726acb0\n",
       "windowSpec3: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5dc3361a\n",
       "windowSpec6: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@747bc527\n",
       "defined class HistData\n",
       "remove_colon_udf: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$2383/0x0000000800e87840@21c827cd,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),None,true,true)\n",
       "add_one_minute_udf: org....\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------+----+---------+--------------------+--------------------+-------+------+\n",
      "|      date|time|new_close|                 ma3|                 ma6|r_close| close|\n",
      "+----------+----+---------+--------------------+--------------------+-------+------+\n",
      "|2024-10-29|1636|    232.5|1.388339337567619...|-3.57633209703833...|    1.0| 232.5|\n",
      "|2024-10-29|1655|   232.46|2.585376759612900...|7.295182104053814E-6|    1.0|232.29|\n",
      "|2024-10-29|1657|   232.56|5.021803641969876E-4|3.301205429367034E-4|    1.0|232.56|\n",
      "|2024-10-29|1706|    232.7|1.433648594277118...| 6.87138792207868E-5|    1.0| 232.6|\n",
      "|2024-10-29|1724|   232.96|2.013605464328272E-4|7.594917581880505E-8|    1.0|232.89|\n",
      "+----------+----+---------+--------------------+--------------------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.catalyst.ScalaReflection\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "//Create three windows (https://spark.apache.org/docs/3.5.0/api/scala/org/apache/spark/sql/expressions/Window$.html)\n",
    "val windowSpec1 = Window.orderBy(col(\"timestamp\")) //Window for 1 period\n",
    "val windowSpec3 = Window.orderBy(col(\"timestamp\")).rowsBetween(-2, 0) //Window for three periods\n",
    "val windowSpec6= Window.orderBy(col(\"timestamp\")).rowsBetween(-5, 0) //Window for 6 periods\n",
    "\n",
    "\n",
    "//Create case class for data input (the same case class will work for both static and streaming data)\n",
    "case class HistData(timestamp: String,open: Double,high: Double, low: Double, close: Double)\n",
    "\n",
    "//Write a udf function remove_colon_udf that drops the colon from a string (e.g., \"9:30\" becomes \"930\")\n",
    "//(the Scala function replace will be useful here)\n",
    "val remove_colon_udf= udf((s: String) => s.replace(\":\",\"\"))\n",
    "\n",
    "/* Write a udf function add_one_minute_udf that adds one minute to a time. For example:\n",
    "930 becomes 931 (add 1)\n",
    "959 becomes 1000 (add 41)\n",
    "\n",
    "*/\n",
    "def add_one_minute_udf = udf((d: String) => {\n",
    "    val value = d.slice(d.length-2,d.length).trim.toInt\n",
    "    if (value < 59) d.trim.toInt + 1\n",
    "    else d.trim.toInt+41\n",
    "})\n",
    "\n",
    "/*\n",
    "\n",
    "Read the static file into a dataframe and do calculations\n",
    "\n",
    "1. timestamp column contains date and time. Drop the date (ex: \"11/4/23 9:30\" becomes \"9:30\")\n",
    "2. using the previously written udf, remove the colon (\"9:30\" becomes \"930\")\n",
    "3. create a new column \"next_timestamp\" using the add_one_minute_udf on timestamp\n",
    "4. calculate r_close. \n",
    "   r_close = (close - open)/(high - low)\n",
    "   r_close indicates how the stock traded. If the close was the high and the open the low, the value is 1 \n",
    "   (i.e., the stock rose steadily over the minute). If the close is the low and the open the hight, the value is -1\n",
    "   (i.e., the stock tanked steadily over the minute)\n",
    "      (IMPORTANT NOTE: If the high==low, the division returns null (not an exception but the identifier\n",
    "   null. You should check if the high==low and return 0 in that case))\n",
    "5. lag the price by 1 minute. \n",
    "    See: https://spark.apache.org/docs/3.5.0/api/java/org/apache/spark/sql/functions.html#lag-org.apache.spark.sql.Column-int-\n",
    "    (use lag(Column e,int offset))\n",
    "    Use windowSpec1 for this)\n",
    "    Call this column p_close (or previous_close)\n",
    "6. Calculate, pct_change, the one minute percent change (close/p_close - 1) \n",
    "7. Calculate ma3, the 3 minute moving average of pct_change \n",
    "    See the example\n",
    "8. Calculate ma6, the 6 minute moving average of pct_change\n",
    "9. select the columns next_timestamp, timestamp, r_close, ma3, ma6 and close\n",
    "    We'll use this for the join with the streaming dataframe\n",
    "    Name the resulting dataframe hist_data\n",
    "\n",
    "*/\n",
    "    \n",
    "val hist_data = spark.read\n",
    "                    .option(\"inferSchema\",true)\n",
    "                    .option(\"header\",true)\n",
    "                    .csv(\"AAPL29.csv\")\n",
    "                    .as[HistData] //Uses the case class to construct the schema\n",
    "                    .select(substring(col(\"timestamp\"),-8,5).as(\"timestamp\"),$\"open\",$\"high\",$\"low\",$\"close\")\n",
    "                    .withColumn(\"timestamp\",remove_colon_udf($\"timestamp\"))\n",
    "                    .withColumn(\"next_timestamp\",add_one_minute_udf($\"timestamp\"))\n",
    "                    .withColumn(\"r_close\",\n",
    "                                (when(col(\"high\")===col(\"low\"),0))\n",
    "                                .otherwise((col(\"close\")-col(\"open\"))/\n",
    "                                          (col(\"high\")-col(\"low\"))))\n",
    "                    .withColumn(\"p_close\", lag(\"close\",1,0).over(windowSpec1))\n",
    "\n",
    "                    .withColumn(\"pct_change\",col(\"close\")/col(\"p_close\")-1)\n",
    "                    .withColumn(\"ma3\",avg(\"pct_change\").over( windowSpec3))\n",
    "                    .withColumn(\"ma6\",avg(\"pct_change\").over( windowSpec6))\n",
    "                    .select(\"next_timestamp\",\"timestamp\",\"r_close\",\"ma3\",\"ma6\",\"close\")\n",
    "\n",
    "/* The stream\n",
    "    * Streaming data is in the file stream_file\n",
    "    * Note that this is not kosher because the timestamps in the stream_file are also in hist_data\n",
    "    * In practice, they won't be (the future) but the assignment then becomes very complicated! \n",
    "\n",
    "1. Create a schema for the streaming data\n",
    "\n",
    "*/\n",
    "\n",
    "val liveStreamSchema = (ScalaReflection\n",
    "                         .schemaFor[HistData]\n",
    "                         .dataType\n",
    "                         .asInstanceOf[StructType])\n",
    "\n",
    "/*\n",
    "2. Manipulate the timestamp to change it to \"0930\" rather than 10/29/2024 9:30\n",
    "3. Rename the timestamp column (so that it won't clash with the timestamp column in hist_data)\n",
    "4. Rename the close column \n",
    "\n",
    "\n",
    "*/\n",
    "val liveDataStream = (spark.readStream\n",
    "                        .schema(liveStreamSchema)\n",
    "                        .option(\"header\",false))\n",
    "                        .option(\"maxFilePerTrigger\",1)\n",
    "                        .csv(\"liveStream\")\n",
    "                        .select(substring(col(\"timestamp\"),0,10).as(\"date\"),\n",
    "                                substring(col(\"timestamp\"),-8,5).as(\"timestamp\"),\n",
    "                                $\"close\")\n",
    "                        .withColumn(\"timestamp\",remove_colon_udf($\"timestamp\"))\n",
    "                        .withColumnRenamed(\"close\",\"new_close\")\n",
    "                        .withColumnRenamed(\"timestamp\",\"time\")\n",
    "\n",
    "/* Join hist_data and liveDataStream and apply the trading rule (this is the query)\n",
    "\n",
    "4. Join hist_data and liveDataStream using next_timestamp from hist_data and the renamed timestamp from\n",
    "liveDataStream (this will join time t from hist_data with time t+1 from the stream)\n",
    "5. Apply the rules:\n",
    "    * ma3 > m6 (both are from hist_data)\n",
    "    * close <= new_close (close from hist_data, new_close from liveDataStream)\n",
    "    * r_close > 0.9 (from hist_data)\n",
    "6. Select the stream timestamp, new close, ma3, ma6 and r_close\n",
    "7. write the stream to the console\n",
    "\n",
    "*/\n",
    "\n",
    "\n",
    "val liveStreamFactors = liveDataStream\n",
    "                       .join(hist_data,hist_data(\"next_timestamp\")===liveDataStream(\"time\"))\n",
    "                        .filter(\"\"\"ma3 > ma6\"\"\")\n",
    "                        .filter(\"\"\"close <= new_close\"\"\")\n",
    "                        .filter(\"\"\"r_close>.9\"\"\")\n",
    "                        .select(\"date\",\"time\",\"new_close\",\"ma3\",\"ma6\",\"r_close\",\"close\")\n",
    "                        .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"console\")\n",
    "    .start\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7cee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "liveStreamFactors.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
